{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model From huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"THUDM/chatglm-6b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert LoRA to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "import loralib as lora\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "              peft_type=\"LORA\", \n",
    "              task_type=\"SEQ_2_SEQ_LM\", \n",
    "              r=8, \n",
    "              lora_alpha=8, \n",
    "              target_modules=[\"q\", \"v\"],\n",
    "              lora_dropout=0.1, \n",
    "              )\n",
    "\n",
    "\n",
    "class QKV_layer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(QKV_layer, self).__init__()\n",
    "        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n",
    "        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n",
    "        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n",
    "\n",
    "    def update(self, target_layer):\n",
    "        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n",
    "        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n",
    "\n",
    "        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n",
    "        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n",
    "\n",
    "        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n",
    "        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        return torch.concat([q,k,v], dim = -1)\n",
    "\n",
    "\n",
    "for key, module in model.named_modules():\n",
    "    if key.endswith('attention'):\n",
    "        try:\n",
    "            # Here we split the query_key_value layer into three linear layer for LoRA.\n",
    "            qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n",
    "            qkv_layer.update(module.query_key_value)\n",
    "            module.query_key_value = qkv_layer\n",
    "        except:\n",
    "            pass\n",
    "        module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n",
    "\n",
    "\n",
    "lora.mark_only_lora_as_trainable(model)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "model_parameters = filter(lambda p: not p.requires_grad, model.parameters())\n",
    "non_trainable_params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_params:3670016 (0.06%), non_trainable_params:6255206400\n"
     ]
    }
   ],
   "source": [
    "print('trainable_params:{} ({:.2f}%), non_trainable_params:{}'.format(trainable_params, trainable_params/non_trainable_params*100,non_trainable_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to train the model with gradient accumulation as well as mix precision, and then save the model (only LoRA's weight which is typically within 10MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 2\n",
    "accumulate_step = 32\n",
    "version = 'test'\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(len(train_dataloader) / accumulate_step),\n",
    "    num_training_steps=(int(len(train_dataloader) / accumulate_step) * NUM_EPOCHS),\n",
    ")\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "with autocast(dtype=torch.bfloat16):\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(t:=tqdm.tqdm(train_dataloader)):\n",
    "            batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss_d = outputs.loss.detach().float()\n",
    "            t.set_description(f\"loss: {loss_d}\")\n",
    "            total_loss += loss_d\n",
    "            loss = outputs.loss / accumulate_step\n",
    "            loss.backward()\n",
    "            if (step+1) % accumulate_step == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        peft_model_id = f\"{checkpoint}_{version}_{epoch}\"\n",
    "        torch.save(lora.lora_state_dict(model), peft_model_id+'.pt')\n",
    "        print(epoch, total_loss/(step+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "# convert it again\n",
    "for key, module in model.named_modules():\n",
    "    if key.endswith('attention'):\n",
    "        try:\n",
    "            qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n",
    "            qkv_layer.update(module.query_key_value)\n",
    "            module.query_key_value = qkv_layer\n",
    "        except:\n",
    "            pass\n",
    "        module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n",
    "\n",
    "\n",
    "# load the LoRA checkpoint\n",
    "model.load_state_dict(torch.load('.pt file you saved'), strict=False)\n",
    "\n",
    "model.half().cuda().eval()\n",
    "\n",
    "# Let's chat!\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
